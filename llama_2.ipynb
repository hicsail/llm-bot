{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d59c916b-398f-4f7f-8758-e60ed917c0ad",
   "metadata": {},
   "source": [
    "# Use of llama 2 through LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcc3d0f7-16b1-4e8e-8a5b-c343e1880af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "llm = Ollama(model=\"llama2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f159429b-4452-47f1-b657-c2ac5c376184",
   "metadata": {},
   "source": [
    "## Using a Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecffd8e8-3c12-4e86-8bd8-9370913081ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.prompts import ChatPromptTemplate\n",
    "# prompt = ChatPromptTemplate.from_messages([\n",
    "#     (\"system\", \"You give medium to short answers.\"), # This is so that my CPU doesn't spike up so much\n",
    "#     (\"user\", \"{input}\")\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e93d5c2-dd8a-4c5b-813e-8725a49f6285",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.chains.prompt_selector import ConditionalPromptSelector\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# prompt = PromptTemplate(\n",
    "#     input_variables=[\"headline\", \"body\"],\n",
    "#     template=\"\"\"<<SYS>> \\n You are an assistant tasked with classifying whether the given \\\n",
    "# headline and body is related to China or not. \\n <</SYS>> \\n\\n [INST] Generate a SHORT response \\\n",
    "# if the given headline and article body is related to China. The output should give a brief explanation of your reasoning. \\n\\n\n",
    "# Headline: \\n\\n {headline} \\n\\n Body: {body} \\n\\n [/INST]\"\"\",\n",
    "# )\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"headline\", \"body\"],\n",
    "    template=\"\"\"<<SYS>> \\n You are an assistant tasked with classifying whether the given \\\n",
    "headline and body is related to China or not. \\n <</SYS>> \\n\\n [INST] Generate a SHORT response \\\n",
    "if the given headline and article body is related to China. The output should be either Yes or No. \\n\\n\n",
    "Headline: \\n\\n {headline} \\n\\n Body: {body} \\n\\n [/INST]\"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75bd369-749e-46ec-bb98-f10925e72a77",
   "metadata": {},
   "source": [
    "## Output Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b47ad49-5b7a-4606-ab67-a807f91ada34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca47448-5106-44bc-8f1e-89fcff4cc719",
   "metadata": {},
   "source": [
    "## LLM Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e8ae96d-1979-41f4-ac07-f62d0c4a2673",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "295f6dfe-a0a2-4e90-a205-2ead0669ee57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#headline = \"Hackers targeted Texas power grid, Hawaii water utility, other critical infrastructure.\"\n",
    "#body = \"A West Coast port and pipeline. A water utility in Hawaii. The Texas power grid. Those are among the recent targets of state-backed Chinese hackers, according to a report published by the Washington Post on Monday. The report cites new information from U.S. officials and industry security officials.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "892db374-0b34-488d-bf14-8d844268b3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "headline = \"Trump appeals Colorado 14th Amendment election disqualification to US Supreme Court.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "901bfe26-ff9b-4d9b-b6a0-4c603da3cbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "body = \"Former President Donald Trump's legal team on Wednesday appealed to the U.S. Supreme Court to overturn the Colorado Supreme Court's ruling disqualifying him from that state's GOP primary ballot, his lawyers confirmed.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "159f8e9e-3812-47bc-9996-8475f0105258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain.invoke({\n",
    "#     \"headline\": headline,\n",
    "#     \"body\": body\n",
    "# })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3daa06c8-b886-4aba-9b85-b49025328bda",
   "metadata": {},
   "source": [
    "# Using llama 2 through GGUF File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99413ef4-611b-4965-872b-f9758f5e2bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/Users/zacharyg/Documents/GitHub/llm-bot/models/llama-2-7b-chat_Q4_K_M.gguf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48ce702e-41a1-43e2-b3b8-ea9eaa430fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from /Users/zacharyg/Documents/GitHub/llm-bot/models/llama-2-7b-chat_Q4_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 3.80 GiB (4.84 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size       =    0.11 MiB\n",
      "ggml_backend_metal_buffer_from_ptr: allocated buffer, size =  3891.95 MiB, ( 3892.02 /  5461.34)\n",
      "llm_load_tensors: system memory used  = 3891.35 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M1\n",
      "ggml_metal_init: picking default device: Apple M1\n",
      "ggml_metal_init: default.metallib not found, loading from source\n",
      "ggml_metal_init: GGML_METAL_PATH_RESOURCES = nil\n",
      "ggml_metal_init: loading '/Users/zacharyg/miniconda3/envs/NLP/lib/python3.9/site-packages/llama_cpp/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M1\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple7 (1007)\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  =  5726.63 MB\n",
      "ggml_metal_init: maxTransferRate               = built-in GPU\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =  1024.00 MiB, ( 4917.58 /  5461.34)\n",
      "llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =     0.02 MiB, ( 4917.59 /  5461.34)\n",
      "llama_build_graph: non-view tensors processed: 676/676\n",
      "llama_new_context_with_model: compute buffer total size = 315.20 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =   312.02 MiB, ( 5229.59 /  5461.34)\n",
      "AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain_community.llms import LlamaCpp\n",
    "\n",
    "llm = LlamaCpp(\n",
    "    model_path=model_path,\n",
    "    n_gpu_layers=1,\n",
    "    n_batch=1024,\n",
    "    n_ctx=2048,\n",
    "    f16_kv=True,\n",
    "    callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11fa32be-fdc7-43af-970d-42f856eb41e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a020a37c-663b-46f5-910c-5281b5f79836",
   "metadata": {},
   "outputs": [],
   "source": [
    "headline = \"Trump appeals Colorado 14th Amendment election disqualification to US Supreme Court.\"\n",
    "body = \"Former President Donald Trump's legal team on Wednesday appealed to the U.S. Supreme Court to overturn the Colorado Supreme Court's ruling disqualifying him from that state's GOP primary ballot, his lawyers confirmed.\"\n",
    "\n",
    "#headline = \"Hackers targeted Texas power grid, Hawaii water utility, other critical infrastructure.\"\n",
    "#body = \"A West Coast port and pipeline. A water utility in Hawaii. The Texas power grid. Those are among the recent targets of state-backed Chinese hackers, according to a report published by the Washington Post on Monday. The report cites new information from U.S. officials and industry security officials.\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e820b3b-ac51-4355-aab7-43a13fdccfd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  No"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   10194.94 ms\n",
      "llama_print_timings:      sample time =       8.17 ms /     3 runs   (    2.72 ms per token,   367.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =   21511.81 ms /    87 tokens (  247.26 ms per token,     4.04 tokens per second)\n",
      "llama_print_timings:        eval time =   25400.49 ms /     2 runs   (12700.24 ms per token,     0.08 tokens per second)\n",
      "llama_print_timings:       total time =   47259.62 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'  No'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\n",
    "    \"headline\": headline,\n",
    "    \"body\": body\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b5f698-fd88-451f-a392-ce7a109f5512",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df006c6-0be8-4b98-aa26-35d697c0b64c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ee4dae-d16b-4dd4-a15f-caa18e968665",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6582841-343b-43fc-af4a-d5bd8a5ef0b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
