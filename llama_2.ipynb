{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d59c916b-398f-4f7f-8758-e60ed917c0ad",
   "metadata": {},
   "source": [
    "# Use of llama 2 through LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc3d0f7-16b1-4e8e-8a5b-c343e1880af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "llm = Ollama(model=\"llama2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f159429b-4452-47f1-b657-c2ac5c376184",
   "metadata": {},
   "source": [
    "## Using a Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecffd8e8-3c12-4e86-8bd8-9370913081ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.prompts import ChatPromptTemplate\n",
    "# prompt = ChatPromptTemplate.from_messages([\n",
    "#     (\"system\", \"You give medium to short answers.\"), # This is so that my CPU doesn't spike up so much\n",
    "#     (\"user\", \"{input}\")\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e93d5c2-dd8a-4c5b-813e-8725a49f6285",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.chains.prompt_selector import ConditionalPromptSelector\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# prompt = PromptTemplate(\n",
    "#     input_variables=[\"headline\", \"body\"],\n",
    "#     template=\"\"\"<<SYS>> \\n You are an assistant tasked with classifying whether the given \\\n",
    "# headline and body is related to China or not. \\n <</SYS>> \\n\\n [INST] Generate a SHORT response \\\n",
    "# if the given headline and article body is related to China. The output should give a brief explanation of your reasoning. \\n\\n\n",
    "# Headline: \\n\\n {headline} \\n\\n Body: {body} \\n\\n [/INST]\"\"\",\n",
    "# )\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"headline\", \"body\"],\n",
    "    template=\"\"\"<<SYS>> \\n You are an assistant tasked with classifying whether the given \\\n",
    "headline and body is related to China or not. \\n <</SYS>> \\n\\n [INST] Generate a SHORT response \\\n",
    "if the given headline and article body is related to China. The output should be either Yes or No. \\n\\n\n",
    "Headline: \\n\\n {headline} \\n\\n Body: {body} \\n\\n [/INST]\"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75bd369-749e-46ec-bb98-f10925e72a77",
   "metadata": {},
   "source": [
    "## Output Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b47ad49-5b7a-4606-ab67-a807f91ada34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca47448-5106-44bc-8f1e-89fcff4cc719",
   "metadata": {},
   "source": [
    "## LLM Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8ae96d-1979-41f4-ac07-f62d0c4a2673",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295f6dfe-a0a2-4e90-a205-2ead0669ee57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#headline = \"Hackers targeted Texas power grid, Hawaii water utility, other critical infrastructure.\"\n",
    "#body = \"A West Coast port and pipeline. A water utility in Hawaii. The Texas power grid. Those are among the recent targets of state-backed Chinese hackers, according to a report published by the Washington Post on Monday. The report cites new information from U.S. officials and industry security officials.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892db374-0b34-488d-bf14-8d844268b3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "headline = \"Trump appeals Colorado 14th Amendment election disqualification to US Supreme Court.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901bfe26-ff9b-4d9b-b6a0-4c603da3cbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "body = \"Former President Donald Trump's legal team on Wednesday appealed to the U.S. Supreme Court to overturn the Colorado Supreme Court's ruling disqualifying him from that state's GOP primary ballot, his lawyers confirmed.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159f8e9e-3812-47bc-9996-8475f0105258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain.invoke({\n",
    "#     \"headline\": headline,\n",
    "#     \"body\": body\n",
    "# })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3daa06c8-b886-4aba-9b85-b49025328bda",
   "metadata": {},
   "source": [
    "# Using llama 2 through GGUF File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99413ef4-611b-4965-872b-f9758f5e2bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./models/llama-2-7b-chat_Q4_K_M.gguf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48ce702e-41a1-43e2-b3b8-ea9eaa430fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from ./models/llama-2-7b-chat_Q4_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 3.80 GiB (4.84 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors: offloading 1 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 1/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  3891.24 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =  1024.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "llama_new_context_with_model:        CPU compute buffer size =   312.01 MiB\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain_community.llms import LlamaCpp\n",
    "\n",
    "llm = LlamaCpp(\n",
    "    model_path=model_path,\n",
    "    n_gpu_layers=1,\n",
    "    n_batch=1024,\n",
    "    n_ctx=2048,\n",
    "    f16_kv=True,\n",
    "    callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9669a9c9-59b1-4fe0-acd2-a496d543229c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.chains.prompt_selector import ConditionalPromptSelector\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "# Prompt\n",
    "# prompt = PromptTemplate(\n",
    "#     input_variables=[\"headline\", \"body\"],\n",
    "#     template=\"\"\"<<SYS>> \\n You are an assistant tasked with classifying whether the given \\\n",
    "# headline and body is related to China or not. \\n <</SYS>> \\n\\n [INST] Generate a SHORT response \\\n",
    "# if the given headline and article body is related to China. The output should be either Yes or No. \\n\\n\n",
    "# Headline: \\n\\n {headline} \\n\\n Body: {body} \\n\\n [/INST]\"\"\",\n",
    "# )\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"headline\", \"body\"],\n",
    "    template=\"\"\"<<SYS>> \\n You are an assistant tasked with classifying whether the given \\\n",
    "headline and body is related to China or not. \\n <</SYS>> \\n\\n [INST] Generate a SHORT response \\\n",
    "if the given headline and article body is related to China. Briefly Explain the reason why.\\n\\n\n",
    "Headline: \\n\\n {headline} \\n\\n Body: {body} \\n\\n [/INST]\"\"\",\n",
    ")\n",
    "\n",
    "\n",
    "# Output Parser\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11fa32be-fdc7-43af-970d-42f856eb41e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a020a37c-663b-46f5-910c-5281b5f79836",
   "metadata": {},
   "outputs": [],
   "source": [
    "#headline = \"Trump appeals Colorado 14th Amendment election disqualification to US Supreme Court.\"\n",
    "#body = \"Former President Donald Trump's legal team on Wednesday appealed to the U.S. Supreme Court to overturn the Colorado Supreme Court's ruling disqualifying him from that state's GOP primary ballot, his lawyers confirmed.\"\n",
    "\n",
    "headline = \"Hackers targeted Texas power grid, Hawaii water utility, other critical infrastructure.\"\n",
    "body = \"A West Coast port and pipeline. A water utility in Hawaii. The Texas power grid. Those are among the recent targets of state-backed Chinese hackers, according to a report published by the Washington Post on Monday. The report cites new information from U.S. officials and industry security officials.\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e820b3b-ac51-4355-aab7-43a13fdccfd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sure! Here's a short response indicating whether the given headline and body is related to China or not:\n",
      "Yes, the article is related to China. According to the report published by the Washington Post, state-backed Chinese hackers have been targeting critical infrastructure in various locations across the United States, including a West Coast port, pipeline, water utility in Hawaii, and the Texas power grid. This suggests that China may be involved in these cyber attacks, given its history of using cyber attacks as a means of espionage and political influence."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   26782.24 ms\n",
      "llama_print_timings:      sample time =      77.46 ms /   119 runs   (    0.65 ms per token,  1536.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =   17629.27 ms /   112 tokens (  157.40 ms per token,     6.35 tokens per second)\n",
      "llama_print_timings:        eval time =   48556.55 ms /   118 runs   (  411.50 ms per token,     2.43 tokens per second)\n",
      "llama_print_timings:       total time =   66746.57 ms /   230 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"  Sure! Here's a short response indicating whether the given headline and body is related to China or not:\\nYes, the article is related to China. According to the report published by the Washington Post, state-backed Chinese hackers have been targeting critical infrastructure in various locations across the United States, including a West Coast port, pipeline, water utility in Hawaii, and the Texas power grid. This suggests that China may be involved in these cyber attacks, given its history of using cyber attacks as a means of espionage and political influence.\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\n",
    "    \"headline\": headline,\n",
    "    \"body\": body\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b5f698-fd88-451f-a392-ce7a109f5512",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df006c6-0be8-4b98-aa26-35d697c0b64c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ee4dae-d16b-4dd4-a15f-caa18e968665",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6582841-343b-43fc-af4a-d5bd8a5ef0b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
